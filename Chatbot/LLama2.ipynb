{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnXKRy21/wwHJp1BR7nson"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"w7QNSFFb5UyL","executionInfo":{"status":"error","timestamp":1715888282525,"user_tz":-60,"elapsed":716,"user":{"displayName":"Carlos Javier Delgado Bolaños","userId":"00080779890986455950"}},"outputId":"75b6cfdc-dfde-4c6a-b317-6ab0167f63e5"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-7e4ca19e9ce0>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Paso 2: Tokenización\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLlamaTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta-llama/Llama-2-7b-chat-hf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Paso 3: Preparación del Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2074\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'meta-llama/Llama-2-7b-chat-hf'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'meta-llama/Llama-2-7b-chat-hf' is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer."]}],"source":["import pandas as pd\n","from transformers import LlamaTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments\n","from torch.utils.data import Dataset\n","import torch\n","\n","# Paso 1: Cargar datos\n","df = pd.read_csv('/content/mental_health_faq.csv')\n","questions = df['Questions'].tolist()\n","answers = df['Answers'].tolist()\n","\n","# Paso 2: Tokenización\n","tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n","\n","# Paso 3: Preparación del Dataset\n","class FAQDataset(Dataset):\n","    def __init__(self, questions, answers, tokenizer, max_len):\n","        self.questions = questions\n","        self.answers = answers\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.questions)\n","\n","    def __getitem__(self, index):\n","        question = self.questions[index]\n","        answer = self.answers[index]\n","\n","        encoding = self.tokenizer(\n","            question,\n","            text_pair=answer,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        input_ids = encoding['input_ids'].squeeze()\n","        attention_mask = encoding['attention_mask'].squeeze()\n","        labels = encoding['input_ids'].squeeze()\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'labels': labels\n","        }\n","\n","dataset = FAQDataset(questions, answers, tokenizer, max_len=512)\n","\n","# Paso 4: Entrenamiento\n","model = LlamaForSequenceClassification.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=4,\n","    save_steps=10_000,\n","    save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n",")\n","\n","trainer.train()\n","\n","# Paso 5: Guardar el modelo\n","model.save_pretrained('./trained_model')\n","tokenizer.save_pretrained('./trained_model')\n","\n","# Paso 6: Configurar el chat para pruebas\n","class ChatBot:\n","    def __init__(self, model_path, tokenizer_path):\n","        self.model = LlamaForSequenceClassification.from_pretrained(model_path)\n","        self.tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)\n","\n","    def ask(self, question):\n","        inputs = self.tokenizer(question, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n","        outputs = self.model(**inputs)\n","        answer = self.tokenizer.decode(torch.argmax(outputs.logits, dim=-1), skip_special_tokens=True)\n","        return answer\n","\n","chatbot = ChatBot('./trained_model', './trained_model')\n","\n","# Ejemplo de uso del chat para pruebas\n","while True:\n","    question = input(\"You: \")\n","    if question.lower() in ['exit', 'quit', 'q']:\n","        break\n","    answer = chatbot.ask(question)\n","    print(f\"Bot: {answer}\")\n"]}]}